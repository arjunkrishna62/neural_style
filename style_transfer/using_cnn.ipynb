{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "IMAGE_SIZE = 256\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferDataset(Dataset):\n",
    "    def __init__(self, content_dir, style_dir):\n",
    "        self.content_dataset = datasets.ImageFolder(content_dir, transform=self.transform())\n",
    "        self.style_dataset = datasets.ImageFolder(style_dir, transform=self.transform())\n",
    "        \n",
    "    def transform(self):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(IMAGE_SIZE),\n",
    "            transforms.CenterCrop(IMAGE_SIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                std=[0.229, 0.224, 0.225])\n",
    "                            \n",
    "        ])\n",
    "    def __len__(self):\n",
    "        return min(len(self.content_dataset), len(self.style_dataset))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        content_img, _ = self.content_dataset[idx]\n",
    "        style_img, _ = self.style_dataset[np.random.randint(0, len(self.style_dataset))]\n",
    "        return content_img, style_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Style transfer blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(128) for _ in range(5)]\n",
    "        )\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 3, 3, padding=1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, content, style):\n",
    "        # Encode both images\n",
    "        content_feat = self.encoder(content)\n",
    "        style_feat = self.encoder(style)\n",
    "        \n",
    "        # AdaIN (Adaptive Instance Normalization)\n",
    "        adain_feat = self.adaptive_instance_normalization(content_feat, style_feat)\n",
    "        \n",
    "        # Decode\n",
    "        out = self.res_blocks(adain_feat)\n",
    "        return self.decoder(out)    \n",
    "    \n",
    "    def adaptive_instance_normalization(self, content, style):\n",
    "        content_mean = torch.mean(content, dim=[2,3], keepdim=True)\n",
    "        content_std = torch.std(content, dim=[2,3], keepdim=True) + 1e-5\n",
    "        style_mean = torch.mean(style, dim=[2,3], keepdim=True)\n",
    "        style_std = torch.std(style, dim=[2,3], keepdim=True) + 1e-5\n",
    "        return style_std * (content - content_mean) / content_std + style_mean\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(channels, channels, 3, padding=1),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "# Loss Functions\n",
    "def gram_matrix(features):\n",
    "    b, c, h, w = features.size()\n",
    "    features = features.view(b, c, h * w)\n",
    "    gram = torch.bmm(features, features.transpose(1, 2))\n",
    "    return gram / (c * h * w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleTransferLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "\n",
    "    def forward(self, generated, content, style):\n",
    "        # Content loss\n",
    "        content_loss = self.mse(generated, content)\n",
    "        \n",
    "        # Style loss\n",
    "        gen_gram = gram_matrix(generated)\n",
    "        style_gram = gram_matrix(style)\n",
    "        style_loss = self.mse(gen_gram, style_gram)\n",
    "        \n",
    "        # Total variation loss\n",
    "        tv_loss = torch.mean(torch.abs(generated[:, :, :, :-1] - generated[:, :, :, 1:])) + \\\n",
    "                  torch.mean(torch.abs(generated[:, :, :-1, :] - generated[:, :, 1:, :]))\n",
    "        \n",
    "        return 1.0 * content_loss + 1e6 * style_loss + 1e-3 * tv_loss\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = StyleTransferDataset(content_dir=\"best-artworks-of-all-time/images\",\n",
    "                              style_dir=\"image-classification/validation\")\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "model = StyleTransferCNN().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "criterion = StyleTransferLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(tensor):\n",
    "    \"\"\"Convert normalized tensors back to PIL images\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
    "    tensor = tensor * std + mean  # denormalize\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return tensor.cpu().detach()\n",
    "\n",
    "def show_images(content, style, generated, epoch, batch_idx):\n",
    "    \"\"\"Display training progress\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Content Image\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(denormalize(content[0]).permute(1, 2, 0))\n",
    "    plt.title(\"Content Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Style Image\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(denormalize(style[0]).permute(1, 2, 0))\n",
    "    plt.title(\"Style Image\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Generated Image\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(denormalize(generated[0]).permute(1, 2, 0))\n",
    "    plt.title(f\"Generated\\nEpoch {epoch+1} Batch {batch_idx}\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "loss_history = []  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (content_imgs, style_imgs) in enumerate(dataloader):\n",
    "        content_imgs = content_imgs.to(device)\n",
    "        style_imgs = style_imgs.to(device)\n",
    "        \n",
    "        # Generate stylized images\n",
    "        generated = model(content_imgs, style_imgs)\n",
    "        \n",
    "        # Calculate features\n",
    "        content_features = model.encoder(content_imgs)\n",
    "        style_features = model.encoder(style_imgs)\n",
    "        generated_features = model.encoder(generated)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(generated_features, content_features, style_features)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{EPOCHS}] Batch [{batch_idx}/{len(dataloader)}] Loss: {loss.item():.4f}\")\n",
    "\n",
    "        # Plot loss after each epoch\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(loss_history, label='Training Loss')\n",
    "        plt.title(\"Training Loss Curve\")\n",
    "        plt.xlabel(\"Batches\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "            # Save model checkpoint\n",
    "        torch.save(model.state_dict(), f\"style_transfer_epoch_{epoch+1}.pth\")\n",
    "\n",
    "# Final loss plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, label='Training Loss')\n",
    "plt.title(\"Final Training Loss Curve\")\n",
    "plt.xlabel(\"Batches\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.savefig(\"training_loss.png\")\n",
    "plt.show()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
